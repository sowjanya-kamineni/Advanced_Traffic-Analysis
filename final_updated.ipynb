{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Car Brand Detection and Classification with YOLOv4**"
      ],
      "metadata": {
        "id": "NZNHmMoU1Iyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall and install compatible versions of NumPy, OpenCV, Gradio, and Matplotlib\n",
        "!pip uninstall numpy opencv-python opencv-contrib-python gradio matplotlib -y\n",
        "!pip install numpy==1.26.4 opencv-contrib-python==4.7.0.72 gradio matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACK10eSBnywT",
        "outputId": "2f908e67-5808-40dd-8843-eed35528621d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: opencv-python 4.11.0.86\n",
            "Uninstalling opencv-python-4.11.0.86:\n",
            "  Successfully uninstalled opencv-python-4.11.0.86\n",
            "Found existing installation: opencv-contrib-python 4.11.0.86\n",
            "Uninstalling opencv-contrib-python-4.11.0.86:\n",
            "  Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
            "\u001b[33mWARNING: Skipping gradio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: matplotlib 3.10.0\n",
            "Uninstalling matplotlib-3.10.0:\n",
            "  Successfully uninstalled matplotlib-3.10.0\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m527.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-contrib-python==4.7.0.72\n",
            "  Downloading opencv_contrib_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, numpy, groovy, ffmpy, aiofiles, starlette, opencv-contrib-python, safehttpx, matplotlib, gradio-client, fastapi, gradio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 matplotlib-3.10.1 numpy-1.26.4 opencv-contrib-python-4.7.0.72 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the runtime to ensure the new versions are loaded\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # This restarts the Colab runtime"
      ],
      "metadata": {
        "id": "JwmscnbP8p4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verify versions\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"OpenCV version:\", cv2.__version__)\n",
        "\n",
        "# Mount Google Drive (only if not already mounted)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths for YOLOv4 files\n",
        "base_path = '/content/drive/MyDrive/Car_Brand_Detection'\n",
        "use_custom_model = False  # Set to True to use yolo-obj.cfg and yolo-obj_final.weights after fixing\n",
        "if use_custom_model:\n",
        "    config_path = f'{base_path}/cfg/yolo-obj.cfg'\n",
        "    weights_path = f'{base_path}/weights/yolo-obj_final.weights'\n",
        "    required_files = {\n",
        "        'yolo-obj.cfg': config_path,\n",
        "        'yolo-obj_final.weights': weights_path,\n",
        "        'obj.names': f'{base_path}/data/obj.names'\n",
        "    }\n",
        "else:\n",
        "    config_path = f'{base_path}/cfg/yolov4.cfg'\n",
        "    weights_path = f'{base_path}/weights/yolov4.weights'\n",
        "    required_files = {\n",
        "        'yolov4.cfg': config_path,\n",
        "        'yolov4.weights': weights_path,\n",
        "        'obj.names': f'{base_path}/data/obj.names'\n",
        "    }\n",
        "labels_path = required_files['obj.names']\n",
        "\n",
        "# Create obj.names with 'car' if not present (for pre-trained model)\n",
        "if not os.path.exists(labels_path):\n",
        "    print(\"[INFO] Creating default obj.names with 'car'...\")\n",
        "    os.makedirs(os.path.dirname(labels_path), exist_ok=True)\n",
        "    with open(labels_path, 'w') as f:\n",
        "        f.write('car\\n')\n",
        "\n",
        "# Verify YOLOv4 files exist\n",
        "missing_files = [name for name, path in required_files.items() if not os.path.exists(path)]\n",
        "if missing_files:\n",
        "    raise FileNotFoundError(f\"Missing files: {missing_files}. Please ensure all files are in the correct Google Drive directories.\")\n",
        "\n",
        "# Create output directories in Google Drive\n",
        "os.makedirs(f'{base_path}/output_video', exist_ok=True)\n",
        "os.makedirs(f'{base_path}/output_charts', exist_ok=True)\n",
        "os.makedirs(f'{base_path}/debug_images', exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "CONFIDENCE = 0.7\n",
        "SCORE_THRESHOLD = 0.7\n",
        "IOU_THRESHOLD = 0.7\n",
        "\n",
        "# Load class labels\n",
        "with open(labels_path, 'r') as f:\n",
        "    LABELS = f.read().strip().split(\"\\n\")\n",
        "print(\"[INFO] Loaded labels:\", LABELS)\n",
        "\n",
        "# Validate .cfg file classes (for custom model)\n",
        "if use_custom_model:\n",
        "    with open(config_path, 'r') as f:\n",
        "        cfg_lines = f.readlines()\n",
        "    cfg_classes = None\n",
        "    for line in cfg_lines:\n",
        "        if line.strip().startswith('classes='):\n",
        "            cfg_classes = int(line.strip().split('=')[1])\n",
        "            break\n",
        "    if cfg_classes != len(LABELS):\n",
        "        raise ValueError(f\"Mismatch between .cfg classes ({cfg_classes}) and obj.names classes ({len(LABELS)}). Update yolo-obj.cfg or obj.names.\")\n",
        "\n",
        "# Generate colors for each class\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
        "\n",
        "# Load YOLO network\n",
        "print(\"[INFO] Loading YOLO from disk...\")\n",
        "try:\n",
        "    net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "except cv2.error as e:\n",
        "    raise Exception(f\"Failed to load YOLOv4 model: {str(e)}. Ensure the .cfg and .weights files are compatible with OpenCV 4.7.0. Try using standard yolov4.cfg and yolov4.weights from https://github.com/AlexeyAB/darknet.\")\n",
        "\n",
        "# Function to generate pie chart\n",
        "def generate_pie_chart(brand_counts, output_path):\n",
        "    if not brand_counts:\n",
        "        return None, \"No vehicles detected for pie chart.\"\n",
        "    labels = list(brand_counts.keys())\n",
        "    counts = list(brand_counts.values())\n",
        "    total = sum(counts)\n",
        "    percentages = [(count / total) * 100 for count in counts]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(counts, labels=[f\"{label} ({percent:.1f}%)\" for label, percent in zip(labels, percentages)],\n",
        "            colors=[plt.cm.tab20(i / len(labels)) for i in range(len(labels))], startangle=140)\n",
        "    plt.title(\"Vehicle Brand Distribution\")\n",
        "    plt.savefig(output_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return output_path, \"Pie chart generated successfully.\"\n",
        "\n",
        "# Function to process image\n",
        "def process_image(image):\n",
        "    if image is None:\n",
        "        return None, None, \"Error: No image provided.\"\n",
        "    # Save uploaded image temporarily\n",
        "    temp_image_path = \"/content/temp_image.jpg\"\n",
        "    cv2.imwrite(temp_image_path, image)\n",
        "    output_path = f\"{base_path}/output_image_yolov4.jpg\"\n",
        "    pie_chart_path = f\"{base_path}/output_charts/image_pie_chart.png\"\n",
        "    debug_path = f\"{base_path}/debug_images/debug_image_{int(time.time())}.jpg\"\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    start = time.time()\n",
        "    layer_outputs = net.forward(ln)\n",
        "    end = time.time()\n",
        "    boxes, confidences, class_ids = [], [], []\n",
        "    for output in layer_outputs:\n",
        "        for detection in output:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > CONFIDENCE:\n",
        "                box = detection[:4] * np.array([w, h, w, h])\n",
        "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                x = int(centerX - (width / 2))\n",
        "                y = int(centerY - (height / 2))\n",
        "                boxes.append([x, y, int(width), int(height)])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "                print(f\"[DEBUG] Detected: Class ID={class_id}, Label={LABELS[class_id]}, Confidence={confidence:.2f}\")\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
        "    font_scale = 1\n",
        "    thickness = 1\n",
        "    font_color = (255, 255, 255)\n",
        "    vehicle_count = len(idxs) if len(idxs) > 0 else 0\n",
        "    brand_counts = Counter(LABELS[class_ids[i]] for i in idxs.flatten()) if len(idxs) > 0 else Counter()\n",
        "    brand_summary = \", \".join(f\"{brand}: {count}\" for brand, count in brand_counts.items()) if brand_counts else \"None\"\n",
        "    if len(idxs) > 0:\n",
        "        for i in idxs.flatten():\n",
        "            x, y = boxes[i][0], boxes[i][1]\n",
        "            w, h = boxes[i][2], boxes[i][3]\n",
        "            color = [int(c) for c in COLORS[class_ids[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
        "            text = f\"{LABELS[class_ids[i]]}: {confidences[i]:.2f}\"\n",
        "            (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
        "            text_offset_x, text_offset_y = x, y - 5\n",
        "            box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
        "            overlay = image.copy()\n",
        "            cv2.rectangle(overlay, box_coords[0], box_coords[1], color, cv2.FILLED)\n",
        "            image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_DUPLEX, font_scale, font_color, thickness)\n",
        "    cv2.imwrite(output_path, image)\n",
        "    cv2.imwrite(debug_path, image)\n",
        "    print(f\"[DEBUG] Saved debug image: {debug_path}\")\n",
        "\n",
        "    # Generate pie chart\n",
        "    pie_chart_output, pie_chart_status = generate_pie_chart(brand_counts, pie_chart_path)\n",
        "\n",
        "    status = (f\"Image processed successfully in {end - start:.2f}s.\\n\"\n",
        "              f\"Vehicles detected: {vehicle_count}\\n\"\n",
        "              f\"Brands detected: {brand_summary}\\n\"\n",
        "              f\"Pie chart status: {pie_chart_status}\")\n",
        "    return output_path, pie_chart_output, status\n",
        "\n",
        "# Function to process video\n",
        "def process_video(video_path):\n",
        "    if video_path is None:\n",
        "        return None, None, \"Error: No video provided.\"\n",
        "    # Define output paths\n",
        "    output_path = f\"{base_path}/output_video/processed_video.avi\"\n",
        "    pie_chart_path = f\"{base_path}/output_charts/video_pie_chart.png\"\n",
        "\n",
        "    vs = cv2.VideoCapture(video_path)\n",
        "    if not vs.isOpened():\n",
        "        return None, None, f\"Error: Could not open video at {video_path}\"\n",
        "    writer = None\n",
        "    (W, H) = (None, None)\n",
        "    brand_counts = Counter()\n",
        "    start = time.time()\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        grabbed, frame = vs.read()\n",
        "        if not grabbed:\n",
        "            break\n",
        "        frame_count += 1\n",
        "        if W is None or H is None:\n",
        "            (H, W) = frame.shape[:2]\n",
        "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "        net.setInput(blob)\n",
        "        layer_outputs = net.forward(ln)\n",
        "        boxes, confidences, class_ids = [], [], []\n",
        "        for output in layer_outputs:\n",
        "            for detection in output:\n",
        "                scores = detection[5:]\n",
        "                class_id = np.argmax(scores)\n",
        "                confidence = scores[class_id]\n",
        "                if confidence > CONFIDENCE:\n",
        "                    box = detection[:4] * np.array([W, H, W, H])\n",
        "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                    x = int(centerX - (width / 2))\n",
        "                    y = int(centerY - (height / 2))\n",
        "                    boxes.append([x, y, int(width), int(height)])\n",
        "                    confidences.append(float(confidence))\n",
        "                    class_ids.append(class_id)\n",
        "                    print(f\"[DEBUG] Frame {frame_count}: Detected: Class ID={class_id}, Label={LABELS[class_id]}, Confidence={confidence:.2f}\")\n",
        "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, SCORE_THRESHOLD, IOU_THRESHOLD)\n",
        "        if len(idxs) > 0:\n",
        "            for i in idxs.flatten():\n",
        "                brand_counts[LABELS[class_ids[i]]] += 1\n",
        "                (x, y) = (boxes[i][0], boxes[i][1])\n",
        "                (w, h) = (boxes[i][2], boxes[i][3])\n",
        "                color = [int(c) for c in COLORS[class_ids[i]]]\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                text = f\"{LABELS[class_ids[i]]}:{confidences[i]:.4f}\"\n",
        "                cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        if writer is None:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "            writer = cv2.VideoWriter(output_path, fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
        "        writer.write(frame)\n",
        "    end = time.time()\n",
        "    print(\"[INFO] Cleaning up...\")\n",
        "    writer.release()\n",
        "    vs.release()\n",
        "\n",
        "    # Generate pie chart\n",
        "    pie_chart_output, pie_chart_status = generate_pie_chart(brand_counts, pie_chart_path)\n",
        "\n",
        "    brand_summary = \", \".join(f\"{brand}: {count}\" for brand, count in brand_counts.items()) if brand_counts else \"None\"\n",
        "    status = (f\"Video processed successfully in {end - start:.2f}s.\\n\"\n",
        "              f\"Brands detected: {brand_summary}\\n\"\n",
        "              f\"Pie chart status: {pie_chart_status}\")\n",
        "    return output_path, pie_chart_output, status\n",
        "\n",
        "# Gradio interface function\n",
        "def car_brand_detection(image, video):\n",
        "    image_output, image_pie_chart, image_status = process_image(image) if image is not None else (None, None, \"No image uploaded.\")\n",
        "    video_output, video_pie_chart, video_status = process_video(video) if video is not None else (None, None, \"No video uploaded.\")\n",
        "    return image_output, image_pie_chart, video_output, video_pie_chart, f\"{image_status}\\n\\n{video_status}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=car_brand_detection,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"numpy\", label=\"Upload Image\"),\n",
        "        gr.Video(label=\"Upload Video\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Processed Image\"),\n",
        "        gr.Image(label=\"Image Vehicle Distribution Pie Chart\"),\n",
        "        gr.Video(label=\"Processed Video\"),\n",
        "        gr.Image(label=\"Video Vehicle Distribution Pie Chart\"),\n",
        "        gr.Textbox(label=\"Processing Status\")\n",
        "    ],\n",
        "    title=\"Car Brand Detection and Classification with YOLOv4\",\n",
        "    description=\"Upload an image and/or video to detect car brands using YOLOv4. Outputs include processed media, detection statistics, and a pie chart showing vehicle brand distribution. Debug images are saved to Google Drive for inspection.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "nDVcmyiCkspP",
        "outputId": "e58a3227-3afc-4c28-d16f-ab148c24eb2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "OpenCV version: 4.7.0\n",
            "[INFO] Loaded labels: ['Audi', 'BMW', 'Bentley', 'Chrysler', 'Ford', 'Honda', 'Hyundai', 'Mercedes-Benz', 'Nissan', 'Toyota']\n",
            "[INFO] Loading YOLO from disk...\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d0af1ffd2c24a5bbb4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d0af1ffd2c24a5bbb4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pq0JFrrlksTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bDDCeAdgkrLL"
      }
    }
  ]
}